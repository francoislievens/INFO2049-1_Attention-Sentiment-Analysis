{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"doc_emb_final.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"zXM7hF2GiCG-"},"source":["#Web and Text Analytics \n","### This Code has been run on colab. This explains commands with \"!\" in front and the drive import \n"," \n","\n","   © Matthias Pirlet, François Lievens, Julien Hubar\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C6HnTZVU2akL","executionInfo":{"status":"ok","timestamp":1637070795456,"user_tz":-60,"elapsed":24091,"user":{"displayName":"Matthias Pirlet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3magscRW6cnVtQlbqSOLWoNN8Hw-C6mUTYOsLaQ=s64","userId":"06284517266189979972"}},"outputId":"3a848baa-d00c-46cb-db83-a9ec8098e43b"},"source":["#import drive \n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QyNc5TTBIsaM","executionInfo":{"status":"ok","timestamp":1637059859556,"user_tz":-60,"elapsed":1110,"user":{"displayName":"Matthias Pirlet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3magscRW6cnVtQlbqSOLWoNN8Hw-C6mUTYOsLaQ=s64","userId":"06284517266189979972"}},"outputId":"eec38a12-cfa6-40e8-9515-dee1f402e4e5"},"source":["!git clone https://github.com/empenguinxh/PyWME"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'PyWME'...\n","remote: Enumerating objects: 22, done.\u001b[K\n","remote: Counting objects: 100% (22/22), done.\u001b[K\n","remote: Compressing objects: 100% (17/17), done.\u001b[K\n","remote: Total 22 (delta 8), reused 14 (delta 4), pack-reused 0\u001b[K\n","Unpacking objects: 100% (22/22), done.\n"]}]},{"cell_type":"markdown","metadata":{"id":"vXDpq-eNBEje"},"source":["/!\\ if you want to use this, you need to modify the path in the file wme.py by \"/content/drive/MyDrive/data/\""]},{"cell_type":"code","metadata":{"id":"nl90PMcJJX30","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9d9f16bd-c095-4545-ba1b-0a35441e766f"},"source":["# Create the numpy array with the document embeddings\n","!python /content/PyWME/wme.py /content/drive/MyDrive/data/data50k_tweet.txt --R 256"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["experiment configuration: Namespace(D_max=6, D_min=1, R=256, corpus_fp='/content/drive/MyDrive/data/data50k_tweet.txt', exp_id='exp_0', gamma=1.0, no_cache=False, nuw_max=500, wv_name='word2vec-google-news-300')\n","... loading word vectors: word2vec-google-news-300 ...\n","tcmalloc: large alloc 3600007168 bytes == 0x560191718000 @  0x7fdb69703001 0x7fdb64fe554f 0x7fdb65035b58 0x7fdb65039b17 0x7fdb650d8203 0x56018f096544 0x56018f096240 0x56018f10a627 0x56018f104ced 0x56018f097bda 0x56018f106737 0x56018f1049ee 0x56018f097bda 0x56018f106737 0x56018f097afa 0x56018f109d00 0x56018f1049ee 0x56018f097bda 0x56018f109d00 0x56018f104ced 0x56018efd6e2b 0x56018f106fe4 0x56018f1049ee 0x56018f1046f3 0x56018f1ce4c2 0x56018f1ce83d 0x56018f1ce6e6 0x56018f1a6163 0x56018f1a5e0c 0x7fdb684ebbf7 0x56018f1a5cea\n","... loading corpus: /content/drive/MyDrive/data/data50k_tweet.txt ...\n","... 50000 docs in total ...\n","... preparing random docs ...\n","... preparing docs ...\n","... computing WME vectors...\n","... load cache for all docs ...\n","saved as /content/drive/MyDrive/data/data50k_tweet_wme_exp_0.npy\n"]}]},{"cell_type":"code","metadata":{"id":"7UR2h2EwYQGB","executionInfo":{"status":"ok","timestamp":1637059868170,"user_tz":-60,"elapsed":5545,"user":{"displayName":"Matthias Pirlet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3magscRW6cnVtQlbqSOLWoNN8Hw-C6mUTYOsLaQ=s64","userId":"06284517266189979972"}}},"source":["import numpy as np\n","#wme_mat is numpy array with the number of line = number of documents\n","wme_mat = np.load('/content/drive/MyDrive/data/data50k_tweet_wme_exp_0.npy')"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1XvgVh2sYU1N","executionInfo":{"status":"ok","timestamp":1637059868171,"user_tz":-60,"elapsed":21,"user":{"displayName":"Matthias Pirlet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3magscRW6cnVtQlbqSOLWoNN8Hw-C6mUTYOsLaQ=s64","userId":"06284517266189979972"}},"outputId":"f998d28d-ed29-4cbc-e03c-abdbf7fa6a64"},"source":["print(wme_mat[420])\n","print(\"Number of documents :\", wme_mat.shape[0])\n","print(\"Size of the embedding :\", wme_mat.shape[1])"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.00231428 0.00239608 0.00255265 0.00249867 0.00257175 0.00246384\n"," 0.00252394 0.00243477 0.00240978 0.00254249 0.00241538 0.00236164\n"," 0.00262419 0.00253007 0.00252735 0.00250423 0.00244535 0.00255964\n"," 0.00247979 0.00255023 0.00237222 0.00253147 0.00247082 0.00255835\n"," 0.00245778 0.00236027 0.00248565 0.00248544 0.00244721 0.00254247\n"," 0.00227787 0.00253978 0.00252791 0.00250085 0.00247678 0.00261112\n"," 0.00246049 0.00247845 0.00249571 0.00249785 0.00244328 0.00255591\n"," 0.00248699 0.00251857 0.00247985 0.00248053 0.00240513 0.0025552\n"," 0.00249902 0.00245256 0.00252311 0.00246118 0.00248712 0.00252399\n"," 0.00248069 0.00251733 0.00246263 0.00248555 0.00236718 0.00254697\n"," 0.00246518 0.00243368 0.00250015 0.00255309 0.00245062 0.00253862\n"," 0.00248576 0.00251824 0.00240345 0.00247724 0.00249546 0.00245356\n"," 0.00248564 0.00242592 0.00247821 0.00240704 0.00248448 0.00243738\n"," 0.00249845 0.00256085 0.00260667 0.00257811 0.00252603 0.00246358\n"," 0.00244035 0.00249852 0.00238015 0.00253373 0.00253589 0.00251536\n"," 0.00258175 0.00247054 0.0025582  0.00249911 0.00242681 0.0024379\n"," 0.00250249 0.0024747  0.0024478  0.00251071 0.00243316 0.00249795\n"," 0.00256571 0.00254823 0.00252001 0.0025535  0.00233147 0.00252616\n"," 0.00254592 0.00253097 0.00253084 0.0025065  0.00260004 0.00251352\n"," 0.00253092 0.00248773 0.00250046 0.00239613 0.00244331 0.00267314\n"," 0.00245139 0.002571   0.00238555 0.0024963  0.00269473 0.00243653\n"," 0.00252588 0.00264198 0.0026323  0.00242513 0.00242595 0.00244556\n"," 0.00242803 0.00248836 0.00233761 0.00245952 0.00235425 0.00252506\n"," 0.00250557 0.00258108 0.00242785 0.00246335 0.00227549 0.00248717\n"," 0.00236628 0.00256547 0.00244848 0.00259478 0.0024708  0.0024756\n"," 0.0025553  0.00244222 0.00254689 0.00253975 0.00245801 0.00256604\n"," 0.00260832 0.00243354 0.00250219 0.00250396 0.0024729  0.00253577\n"," 0.00234099 0.00253545 0.00232381 0.00253406 0.00252142 0.00259339\n"," 0.00244028 0.00246874 0.00244262 0.00254394 0.00250922 0.00239458\n"," 0.0024011  0.00249617 0.00256445 0.00249018 0.00247649 0.00255937\n"," 0.00252194 0.0023904  0.00236817 0.00241065 0.00255208 0.00247324\n"," 0.00253769 0.00241553 0.00238237 0.00253741 0.00248744 0.00247599\n"," 0.00250156 0.00255025 0.00245048 0.00260227 0.00257488 0.00251283\n"," 0.00256693 0.00257465 0.00235199 0.00252936 0.0026233  0.00251575\n"," 0.00250544 0.00253361 0.00260534 0.00247589 0.00253012 0.00256422\n"," 0.00256842 0.00246181 0.00254654 0.00259786 0.00251819 0.00239877\n"," 0.00253254 0.00254216 0.00247422 0.00247312 0.002631   0.00244616\n"," 0.00257321 0.00248403 0.00249308 0.00234995 0.00251641 0.00239148\n"," 0.00249725 0.00243714 0.00251333 0.00249483 0.00246425 0.00250848\n"," 0.0024508  0.00248437 0.00254949 0.00251168 0.00252683 0.00248242\n"," 0.00245429 0.00246187 0.00242622 0.00245137 0.00248724 0.00235725\n"," 0.0025379  0.0024878  0.00250951 0.00241052 0.00248671 0.0024774\n"," 0.00253118 0.00258761 0.0025098  0.00247319]\n","Number of documents : 50000\n","Size of the embedding : 256\n"]}]},{"cell_type":"code","metadata":{"id":"f2rOr4Lhid_u","executionInfo":{"status":"ok","timestamp":1637059900627,"user_tz":-60,"elapsed":26390,"user":{"displayName":"Matthias Pirlet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3magscRW6cnVtQlbqSOLWoNN8Hw-C6mUTYOsLaQ=s64","userId":"06284517266189979972"}}},"source":["import torch\n","import torch.nn as nn\n","\n","def init_weights(m):\n","    if isinstance(m, nn.Linear):\n","        torch.nn.init.xavier_uniform_(m.weight)\n","        m.bias.data.fill_(0.01)\n","\n","\n","class Feedforward(nn.Module):\n","  def __init__(self, input_size, hidden_size_1, hidden_size_2, hidden_size_3):\n","    super(Feedforward, self).__init__()\n","\n","    self.mlp = nn.Sequential(nn.Linear(input_size, hidden_size_1), nn.ReLU(),\n","                             nn.Linear(hidden_size_1, hidden_size_2), nn.ReLU(),\n","                             nn.Linear(hidden_size_2,hidden_size_3), nn.ReLU(),\n","                             nn.Linear(hidden_size_3, 1), nn.Sigmoid())\n","    \n","    self.mlp.apply(init_weights)\n","\n","  def forward(self, x):\n","    output = self.mlp(x.float())\n","    return output"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"mWpHtzD135MP","executionInfo":{"status":"ok","timestamp":1637059900629,"user_tz":-60,"elapsed":16,"user":{"displayName":"Matthias Pirlet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3magscRW6cnVtQlbqSOLWoNN8Hw-C6mUTYOsLaQ=s64","userId":"06284517266189979972"}}},"source":["import os\n","import random\n","from torch.utils.data import Dataset\n","from pandas import *\n","\n","\n","def generateDataset(root_dir, train, seed):\n","  \n","  #get labels into a list\n","  label_path = os.path.join(root_dir, \"data50k_tweet.csv\")\n","  data = read_csv(label_path)\n","  label_list = data['sentiment'].tolist()  \n","\n","  #get tweets into a list\n","  tweet_path = os.path.join(root_dir, \"data50k_tweet_wme_exp_0.npy\")\n","  wme_mat = np.load(tweet_path)\n","  \n","  dataset = []\n","  for i in range(len(label_list)):\n","    dataset.append((label_list[i], wme_mat[i]))\n","  random.seed(seed)\n","  random.shuffle(dataset)\n","\n","\n","  if train:\n","    dataset = dataset[10000:]\n","  else:\n","    dataset = dataset[:10000]\n","\n","  return dataset\n","\n","\n","class tweetDataset(Dataset):\n","  \n","  def __init__(self, root_dir, train=True, seed=0):\n","    self.root_dir = root_dir\n","    self.dataset = generateDataset(root_dir, train, seed)\n","\n","\n","  def __len__(self):\n","    return len(self.dataset)\n","\n","  \n","  def __getitem__(self, idx):\n","    return self.dataset[idx]"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gFyKPfbKjF_F","executionInfo":{"status":"ok","timestamp":1637059902155,"user_tz":-60,"elapsed":1541,"user":{"displayName":"Matthias Pirlet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3magscRW6cnVtQlbqSOLWoNN8Hw-C6mUTYOsLaQ=s64","userId":"06284517266189979972"}},"outputId":"033e1689-9747-475b-cdd7-d2000deb571b"},"source":["my_dataset = tweetDataset(\"/content/drive/MyDrive/data\") # training directory\n","my_loader = torch.utils.data.DataLoader(my_dataset, batch_size=50, shuffle=True, num_workers=0)\n","\n","dataiter = iter(my_loader)\n","sentiment, tweet = dataiter.next()\n","print(sentiment)\n","print(tweet)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n","        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,\n","        1, 0])\n","tensor([[0.0052, 0.0055, 0.0057,  ..., 0.0057, 0.0055, 0.0055],\n","        [0.0038, 0.0040, 0.0040,  ..., 0.0041, 0.0040, 0.0040],\n","        [0.0034, 0.0035, 0.0036,  ..., 0.0037, 0.0036, 0.0036],\n","        ...,\n","        [0.0053, 0.0055, 0.0056,  ..., 0.0057, 0.0055, 0.0055],\n","        [0.0033, 0.0033, 0.0034,  ..., 0.0035, 0.0034, 0.0033],\n","        [0.0043, 0.0046, 0.0047,  ..., 0.0047, 0.0047, 0.0045]],\n","       dtype=torch.float64)\n"]}]},{"cell_type":"code","metadata":{"id":"P8DOVmSHxZiP","executionInfo":{"status":"ok","timestamp":1637059902156,"user_tz":-60,"elapsed":5,"user":{"displayName":"Matthias Pirlet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3magscRW6cnVtQlbqSOLWoNN8Hw-C6mUTYOsLaQ=s64","userId":"06284517266189979972"}}},"source":["# Device configuration\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"PLSZNhn1bcg-","executionInfo":{"status":"ok","timestamp":1637059902157,"user_tz":-60,"elapsed":5,"user":{"displayName":"Matthias Pirlet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3magscRW6cnVtQlbqSOLWoNN8Hw-C6mUTYOsLaQ=s64","userId":"06284517266189979972"}}},"source":["def save_model(model, optimizer, model_path, model_name):\n","\n","    torch.save(model.state_dict(), '{}/{}/model_weights.pt'.format(model_path, model_name))\n","    torch.save(optimizer.state_dict(), '{}/{}/optimizer_weights.pt'.format(model_path, model_name))\n","\n","def save_logs(epoch, loss, accuracy, type='train'):\n","\n","    if type=='train':\n","      f = open('/content/drive/MyDrive/data/results_train.csv', 'a')\n","    elif type=='test':\n","      f = open('/content/drive/MyDrive/data/results_test.csv', 'a')\n","    for i in range(0, len(loss)):\n","        f.write('{},{},{}\\n'.format(epoch, loss[i], accuracy[i]))\n","    f.close()"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"ccRcYSFblHRF","executionInfo":{"status":"ok","timestamp":1637059902157,"user_tz":-60,"elapsed":5,"user":{"displayName":"Matthias Pirlet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3magscRW6cnVtQlbqSOLWoNN8Hw-C6mUTYOsLaQ=s64","userId":"06284517266189979972"}}},"source":["def accuracy_func(pred, target):\n","    rounded = torch.round(pred)\n","    # Get correct predictions\n","    accu = sum(rounded == target for rounded, target in zip(rounded, target)) / pred.shape[0]\n","    return accu\n","\n","def train(model, loaders, criterion, optimizer, epochs):\n","    \n","    total_batches = len(loaders[\"train\"]) * epochs\n","    test_loss_min = np.Inf\n","    batch_ct = 0\n","    sample_ct = 0\n","    \n","    for epoch in range(epochs):\n","        train_loss = 0.0\n","        test_loss = 0.0\n","        accuracy = 0.0\n","\n","        # Train the model\n","        model.train()\n","        train_loss_list = []\n","        train_acc_list = []\n","        test_loss_list = []\n","        test_acc_list = []\n","\n","        for batch_idx, (targets, inputs) in enumerate(loaders[\"train\"]):\n","            # Send them to the GPU\n","            inputs, targets = inputs.to(device), targets.to(device)\n","\n","            \n","            # optimizer.zero_grad()\n","            for param in model.parameters(): # Alternative recommended\n","                param.grad = None\n","            \n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets.unsqueeze(1).type(torch.FloatTensor).to(device))\n","            loss.backward()\n","            optimizer.step()\n","\n","            sample_ct += len(inputs)\n","            batch_ct += 1\n","\n","            train_loss_list.append(loss.item())\n","\n","            acc = accuracy_func(outputs, targets)\n","            train_acc_list.append(acc.item())\n","\n","            train_loss += (1 / (batch_idx + 1)) * (loss.data - train_loss)\n","\n","        #open file csv, for i in len(train_loss):\n","        # mettre dans le csv \n","        save_logs(epoch, train_loss_list, train_acc_list, 'train')\n","\n","        # Evaluate the model\n","        model.eval()\n","        with torch.no_grad():\n","            total = 0\n","            correct = 0\n","            for batch_idx, (targets, inputs) in enumerate(loaders[\"test\"]):\n","                # Send them to the GPU\n","                inputs, targets = inputs.to(device), targets.to(device)\n","                total += targets.size(0)\n","\n","                outputs = model(inputs)\n","                loss = criterion(outputs, targets.unsqueeze(1).to(torch.float))\n","                test_loss += (1 / (batch_idx + 1)) * (loss.data - test_loss)\n","\n","                test_loss_list.append(loss.item())\n","\n","                acc = accuracy_func(outputs, targets)\n","                test_acc_list.append(acc.item())\n","\n","                predicted = torch.where(outputs > 0.5, 1, 0)\n","                correct += (predicted == targets.unsqueeze(1)).sum().item()\n","\n","            save_logs(epoch, test_loss_list, test_acc_list, 'test')\n","\n","        # Log it\n","        accuracy = 100 * correct / total\n","        #to_log = {\"epoch\": epoch,\n","        #         \"training loss\": train_loss,\n","        #         \"testing loss\": test_loss,\n","        #         \"accuracy\": accuracy,\n","        #         \"time\": time.time()-start}\n","        #wandb.log(to_log, step=sample_ct)\n","        print(\"Epoch: {} \\tTraining Loss: {:.5f} \\tTest Loss: {:.5f} \\tAccuracy: {:.5f}%\".format(epoch, train_loss, test_loss, accuracy))\n","        \n","        # Save the model\n","        if test_loss <= test_loss_min:\n","        #if False:\n","            print('Test loss decreased ({:.5f} --> {:.5f}). Saving model...\\n'.format(test_loss_min, test_loss))\n","            #torch.onnx.export(model, inputs, \"model.onnx\")\n","            #wandb.save(\"model.onnx\")\n","            test_loss_min = test_loss\n","\n","    return model    "],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zENRY6iryf28","executionInfo":{"status":"ok","timestamp":1637059907457,"user_tz":-60,"elapsed":455,"user":{"displayName":"Matthias Pirlet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3magscRW6cnVtQlbqSOLWoNN8Hw-C6mUTYOsLaQ=s64","userId":"06284517266189979972"}},"outputId":"7394fd7c-4f21-40a2-dd21-a232532407dc"},"source":["### First hyperparameters ###\n","num_workers = 6\n","batch_size = 8\n","\n","# Loaders\n","training_dataset = tweetDataset(\"/content/drive/MyDrive/data\", train=True)\n","testing_dataset = tweetDataset(\"/content/drive/MyDrive/data\", train=False)\n","\n","train_loader = torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True,\n","                                           pin_memory=True, num_workers=num_workers)\n","test_loader = torch.utils.data.DataLoader(testing_dataset, batch_size=batch_size, shuffle=True,\n","                                          pin_memory=True, num_workers=num_workers)\n","\n","loaders = {\n","    \"train\": train_loader,\n","    \"test\": test_loader\n","}"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0NBmgyoczAD1","executionInfo":{"status":"ok","timestamp":1637061920010,"user_tz":-60,"elapsed":833106,"user":{"displayName":"Matthias Pirlet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3magscRW6cnVtQlbqSOLWoNN8Hw-C6mUTYOsLaQ=s64","userId":"06284517266189979972"}},"outputId":"0f67f486-7f78-4ec0-8531-87e61f881290"},"source":["#momentum=0.005\n","learning_rate=1e-4\n","\n","model = Feedforward(256, 128, 64, 32).to(device)\n","# Binary Cross Entropy Loss\n","criterion = nn.BCELoss()\n","#optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","print(model)\n","    \n","# Train it\n","model_f = train(model, loaders, criterion, optimizer, epochs=25)\n"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Feedforward(\n","  (mlp): Sequential(\n","    (0): Linear(in_features=256, out_features=128, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=128, out_features=64, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=64, out_features=32, bias=True)\n","    (5): ReLU()\n","    (6): Linear(in_features=32, out_features=1, bias=True)\n","    (7): Sigmoid()\n","  )\n",")\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 0 \tTraining Loss: 0.66754 \tTest Loss: 0.65499 \tAccuracy: 61.30000%\n","Test loss decreased (inf --> 0.65499). Saving model...\n","\n","Epoch: 1 \tTraining Loss: 0.65411 \tTest Loss: 0.65279 \tAccuracy: 60.98000%\n","Test loss decreased (0.65499 --> 0.65279). Saving model...\n","\n","Epoch: 2 \tTraining Loss: 0.65226 \tTest Loss: 0.65345 \tAccuracy: 61.36000%\n","Epoch: 3 \tTraining Loss: 0.65097 \tTest Loss: 0.65058 \tAccuracy: 61.43000%\n","Test loss decreased (0.65279 --> 0.65058). Saving model...\n","\n","Epoch: 4 \tTraining Loss: 0.64939 \tTest Loss: 0.64885 \tAccuracy: 61.72000%\n","Test loss decreased (0.65058 --> 0.64885). Saving model...\n","\n","Epoch: 5 \tTraining Loss: 0.64812 \tTest Loss: 0.64838 \tAccuracy: 61.40000%\n","Test loss decreased (0.64885 --> 0.64838). Saving model...\n","\n","Epoch: 6 \tTraining Loss: 0.64752 \tTest Loss: 0.64989 \tAccuracy: 61.54000%\n","Epoch: 7 \tTraining Loss: 0.64582 \tTest Loss: 0.66377 \tAccuracy: 60.33000%\n","Epoch: 8 \tTraining Loss: 0.64437 \tTest Loss: 0.65108 \tAccuracy: 60.80000%\n","Epoch: 9 \tTraining Loss: 0.64251 \tTest Loss: 0.64867 \tAccuracy: 60.95000%\n","Epoch: 10 \tTraining Loss: 0.63960 \tTest Loss: 0.63625 \tAccuracy: 63.16000%\n","Test loss decreased (0.64838 --> 0.63625). Saving model...\n","\n","Epoch: 11 \tTraining Loss: 0.63511 \tTest Loss: 0.64010 \tAccuracy: 62.23000%\n","Epoch: 12 \tTraining Loss: 0.62836 \tTest Loss: 0.62012 \tAccuracy: 64.63000%\n","Test loss decreased (0.63625 --> 0.62012). Saving model...\n","\n","Epoch: 13 \tTraining Loss: 0.61335 \tTest Loss: 0.59886 \tAccuracy: 69.76000%\n","Test loss decreased (0.62012 --> 0.59886). Saving model...\n","\n","Epoch: 14 \tTraining Loss: 0.58852 \tTest Loss: 0.56311 \tAccuracy: 75.65000%\n","Test loss decreased (0.59886 --> 0.56311). Saving model...\n","\n","Epoch: 15 \tTraining Loss: 0.54845 \tTest Loss: 0.51756 \tAccuracy: 75.27000%\n","Test loss decreased (0.56311 --> 0.51756). Saving model...\n","\n","Epoch: 16 \tTraining Loss: 0.48454 \tTest Loss: 0.45796 \tAccuracy: 76.39000%\n","Test loss decreased (0.51756 --> 0.45796). Saving model...\n","\n","Epoch: 17 \tTraining Loss: 0.40884 \tTest Loss: 0.38605 \tAccuracy: 86.78000%\n","Test loss decreased (0.45796 --> 0.38605). Saving model...\n","\n","Epoch: 18 \tTraining Loss: 0.33536 \tTest Loss: 0.28019 \tAccuracy: 95.45000%\n","Test loss decreased (0.38605 --> 0.28019). Saving model...\n","\n","Epoch: 19 \tTraining Loss: 0.28082 \tTest Loss: 0.23263 \tAccuracy: 94.31000%\n","Test loss decreased (0.28019 --> 0.23263). Saving model...\n","\n","Epoch: 20 \tTraining Loss: 0.23675 \tTest Loss: 0.19012 \tAccuracy: 97.04000%\n","Test loss decreased (0.23263 --> 0.19012). Saving model...\n","\n","Epoch: 21 \tTraining Loss: 0.20748 \tTest Loss: 0.21035 \tAccuracy: 90.63000%\n","Epoch: 22 \tTraining Loss: 0.18857 \tTest Loss: 0.26127 \tAccuracy: 87.79000%\n","Epoch: 23 \tTraining Loss: 0.17972 \tTest Loss: 0.12839 \tAccuracy: 97.12000%\n","Test loss decreased (0.19012 --> 0.12839). Saving model...\n","\n","Epoch: 24 \tTraining Loss: 0.16767 \tTest Loss: 0.11744 \tAccuracy: 97.31000%\n","Test loss decreased (0.12839 --> 0.11744). Saving model...\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"uHs2iRXpkAfX"},"source":["### This had to be run on our PC because Colab doesn't accept to plot and save the figures"]},{"cell_type":"code","metadata":{"id":"rCjtfLd1fv-n","executionInfo":{"status":"ok","timestamp":1637074159465,"user_tz":-60,"elapsed":385,"user":{"displayName":"Matthias Pirlet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3magscRW6cnVtQlbqSOLWoNN8Hw-C6mUTYOsLaQ=s64","userId":"06284517266189979972"}}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def get_epoch_idx(seq):\n","\n","    epoch_idx = []\n","    start_epoch = 0\n","    for i in range(len(seq)):\n","        if int(seq[i]) > start_epoch:\n","            epoch_idx.append(i)\n","            start_epoch = int(seq[i])\n","\n","    return epoch_idx\n","\n","\n","def avg_smoothing(seq, window_size):\n","\n","    start_idx = 0\n","    end_idx = window_size\n","    target_idx = int(window_size / 2)\n","    output = seq.copy()\n","\n","    while end_idx < len(seq):\n","        output[target_idx] = np.mean(seq[start_idx:end_idx])\n","        start_idx += 1\n","        end_idx += 1\n","        target_idx += 1\n","\n","    return output\n","\n","\n","idx = 0\n","epoch_idx = []\n","\n","#parameters\n","smoothing_widow = 1000\n","alf_window = int(smoothing_widow / 2)\n","column_names = [\"Epoch\", \"loss\", \"accuracy\"]\n","red = 'tab:red'\n","blue = 'tab:blue'\n","\n","#read CSV files\n","train_logs = pd.read_csv(\n","  './results_train.csv',\n","  header=None,\n","  names = column_names,\n","  index_col=False,\n","  encoding='latin')\n","\n","#problem when reading first row of first column\n","train_logs['Epoch'] = train_logs['Epoch'].replace(['ï»¿0'],'0')\n","\n","#to plot 2 variables on same graph\n","fig, ax1 = plt.subplots()\n","ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n","\n","#first variable \n","loss = train_logs['loss'].to_numpy()\n","loss = avg_smoothing(loss, smoothing_widow)\n","loss = loss[alf_window:-alf_window]\n","\n","ax1.plot(np.arange(len(loss)), loss,\n","                    label =\"Train loss\",\n","                    linewidth=0.3,\n","                    color=red)\n","ax1.set_ylabel('Loss')\n","\n","\n","#second variable\n","acc = train_logs['accuracy'].to_numpy()\n","acc = avg_smoothing(acc, smoothing_widow)\n","acc = acc[alf_window:-alf_window]\n","\n","ax2.plot(np.arange(len(acc)), acc,\n","                     label=\"Train accuracy\",\n","                     linewidth=0.3,\n","                     color=blue)\n","ax2.set_ylabel('Accuracy')\n","\n","\n","#get epochs\n","epoch_idx = get_epoch_idx(train_logs['Epoch'].to_numpy())\n","\n","[plt.axvline(x=epoch_idx[i], color='black',linewidth=0.2, linestyle='-')\n","     for i in range(0, len(epoch_idx))]\n","\n","lines, labels = ax1.get_legend_handles_labels()\n","lines2, labels2 = ax2.get_legend_handles_labels()\n","ax2.legend(lines + lines2, labels + labels2, loc='center right')\n","\n","plt.title('Training logs')\n","ax1.set_xlabel('Batch')\n","plt.savefig('./train_perfs.png', format='png')\n","plt.show()"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"hiyz-q86i5iw"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def get_epoch_idx(seq):\n","\n","    epoch_idx = []\n","    start_epoch = 0\n","    for i in range(len(seq)):\n","        if int(seq[i]) > start_epoch:\n","            epoch_idx.append(i)\n","            start_epoch = int(seq[i])\n","\n","    return epoch_idx\n","\n","\n","def avg_smoothing(seq, window_size):\n","\n","    start_idx = 0\n","    end_idx = window_size\n","    target_idx = int(window_size / 2)\n","    output = seq.copy()\n","\n","    while end_idx < len(seq):\n","        output[target_idx] = np.mean(seq[start_idx:end_idx])\n","        start_idx += 1\n","        end_idx += 1\n","        target_idx += 1\n","\n","    return output\n","\n","\n","idx = 0\n","epoch_idx = []\n","\n","#parameters\n","smoothing_widow = 200\n","alf_window = int(smoothing_widow / 2)\n","column_names = [\"Epoch\", \"loss\", \"accuracy\"]\n","red = 'tab:red'\n","blue = 'tab:blue'\n","\n","#read CSV files\n","test_logs = pd.read_csv(\n","  '/content/drive/MyDrive/data/results_test.csv',\n","  header=None,\n","  names = column_names,\n","  index_col=False,\n","  encoding='latin')\n","\n","#problem when reading first row of first column\n","test_logs['Epoch'] = test_logs['Epoch'].replace(['ï»¿0'],'0')\n","\n","\n","\n","\n","#to plot 2 variables on same graph\n","fig, ax1 = plt.subplots()\n","ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n","\n","#first variable \n","loss = test_logs['loss'].to_numpy()\n","loss = avg_smoothing(loss, smoothing_widow)\n","loss = loss[alf_window:-alf_window]\n","\n","ax1.plot(np.arange(len(loss)), loss,\n","                    label =\"Test loss\",\n","                    linewidth=0.3,\n","                    color=red)\n","ax1.set_ylabel('Loss')\n","\n","\n","#second variable\n","acc = test_logs['accuracy'].to_numpy()\n","acc = avg_smoothing(acc, smoothing_widow)\n","acc = acc[alf_window:-alf_window]\n","\n","ax2.plot(np.arange(len(acc)), acc,\n","                     label=\"Test accuracy\",\n","                     linewidth=0.3,\n","                     color=blue)\n","ax2.set_ylabel('Accuracy')\n","\n","\n","#get epochs\n","epoch_idx = get_epoch_idx(test_logs['Epoch'].to_numpy())\n","\n","[plt.axvline(x=epoch_idx[i], color='black',linewidth=0.2, linestyle='-')\n","     for i in range(0, len(epoch_idx))]\n","\n","lines, labels = ax1.get_legend_handles_labels()\n","lines2, labels2 = ax2.get_legend_handles_labels()\n","ax2.legend(lines + lines2, labels + labels2, loc='center right')\n","\n","plt.title('Test logs')\n","ax1.set_xlabel('Batch')\n","plt.savefig('/content/drive/MyDrive/data/test_perfs.png', format='png')\n","plt.show()"],"execution_count":null,"outputs":[]}]}